# Maths-CS-&-ML-Papers
A collection of my favourite papers (in no particular order) I have read in deep learning. As of now, I am inspired by research in epidemiology, spiking neural networks 
(SNNs), meta-learning and self-pruning networks. The analysis of each paper follows a structure resembling: 
- Title and authors
- Context/history
- Problems adressed
- Main contributions
- Strengths
- Weaknesses or limitations
- My takeaways.

# Wide-Impact Papers
### A collection of papers which had paradigm-shifting effects in research and or industry, with legendary status.
[COMPUTING, MACHINERY AND INTELLIGENCE](https://doi.org/10.1093/mind/LIX.236.433) (Alan Mathson Turing)


[ImageNet classification with deep convolutional neural networks](https://doi.org/10.1145/3065386) (Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton)
The paper is synonymous with the modern deep learning revolution, it demonstrated the power of deep convolutional neural networks (CNNs), achieving performance on the ImageNet dataset previously unheard of. AlexNet was key in sparking widespread interest and research funding investments in deep learning.



brought artificial intelligence research out of its second AI winter, revolutionising machine learning with its 'deep', layered approach.

[Attention is all you need](https://doi.org/10.48550/arXiv.1706.03762) (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin)
The paper responisble for the LLMs today, the transformer architecture was and still is a revolutionary jump in language processing, ..., ...

[Long Short-Term Memory](https://doi.org/10.1162/neco.1997.9.8.1735) (Sepp Hochreiter, JÃ¼rgen Schmidhuber)
A highly influential architecture which eventually led to the transformer (from paper above)

[]() (John von Neumann)
This paper 


[A Mathematical Theory of Communication](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) (Claude Shannon)
In this paper Shannon essentially established the field of Information Therory (there were earlier works in the 1920's). I first encountered Shannon's work in biology _Shannon's Diversity index_ is originally a equation in entropy , which actually was defined in this very paper.


# Meta-Learning Papers
[Meta-Learning in Neural Networks: A Survey](https://arxiv.org/pdf/2004.05439) (Timothy Hospedales, Antreas Antoniou, Paul Micaelli, Amos Storkey)

Hospedales et. al describe Meta-learning as a potential candidate to combat the data inefficiencies, poor knowledge transfer and unsupervised learning aspects of DNNs in research at the moment. There are different interpretations of the phrase 'meta-learning' but the paper focuses on **contemporary *neural network* meta-learning**. Meaning "algorithm learning, but focus on where this is achieved by end-to-end learning of an explicitly defined objective function (such as cross-entropy loss)".


# Dissertation Papers (ATT-GCN-LSTM / COVID19)
### This section reads in a rough order of increasing complexity


[Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting](https://ojs.aaai.org/index.php/AAAI/article/view/3881) (Guo S, Lin Y, Feng N, Song C, Wan H) 

[Short-Term Multi-Horizon Line Loss Rate Forecasting of a Distribution Network Using Attention-GCN-LSTM](https://arxiv.org/abs/2312.11898) (Liu J, Cao Y, Li Y, Guo Y, Deng W) 

[Integrating LSTMs and GNNs for COVID-19 Forecasting](https://arxiv.org/abs/2108.10052) (Sesti N, Garau-Luis JJ, Crawley E, Cameron B) 

[Predicting COVID-19 positivity and hospitalization with multi-scale graph neural networks](https://doi.org/10.1038/s41598-023-31222-6) (Skianis K, Nikolentzos G, Gallix B, Thiebaut R, Exarchakis G)


[Attention-based LSTM predictive model for the attitude and position of shield machine in tunneling](https://www.sciencedirect.com/science/article/pii/S2467967423000880) (Kang Q, Chen EJ, Li ZC, Luo HB, Liu Y)
















